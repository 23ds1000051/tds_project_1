# -*- coding: utf-8 -*-
"""GitHub Scrapper

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jpYiO-UqFofJtgcvnHeCSf8aU7CJeGNN
"""

import requests
from google.colab import userdata
import csv

# Retrieve API key securely from Google Colab's userdata
api_key = userdata.get('GITHUB_KEY')

def clean_company_name(company):
    """Clean up company names: remove leading @, trim whitespace, convert to uppercase."""
    if company:
        return company.strip().lstrip('@').upper()
    return ""

def search_github_users(location, followers, token=None):
    """Search GitHub users by location and minimum followers."""
    users_data = []
    repos_data = []
    page = 1

    while page <= 10:  # Increase the number of pages to fetch more users
        url = f"https://api.github.com/search/users?q=location:{location}+followers:>{followers}&page={page}&per_page=100"  # Fetch 100 users per page
        headers = {"Accept": "application/vnd.github+json"}

        if token:
            headers["Authorization"] = f"Bearer {token}"

        response = requests.get(url, headers=headers)

        if response.status_code == 200:
            data = response.json()
            for user in data['items']:
                # Fetch detailed user info
                user_details_url = f"https://api.github.com/users/{user['login']}"
                user_details_response = requests.get(user_details_url, headers=headers)

                if user_details_response.status_code == 200:
                    user_details = user_details_response.json()

                    # Extract and clean user data
                    cleaned_company = clean_company_name(user_details.get('company', ''))

                    users_data.append([
                        user_details.get('login', ''),
                        user_details.get('name', ''),
                        cleaned_company,
                        user_details.get('location', ''),
                        user_details.get('email', ''),
                        user_details.get('hireable', False),
                        user_details.get('bio', ''),
                        user_details.get('public_repos', 0),
                        user_details.get('followers', 0),
                        user_details.get('following', 0),
                        user_details.get('created_at', '')
                    ])

                    # Fetch user repositories
                    user_repos_url = user_details.get('repos_url', '')
                    repos_data.extend(fetch_user_repositories(user_details['login'], user_repos_url, headers))
                else:
                    print(f"Failed to fetch details for {user['login']}: {user_details_response.status_code}")

            if not data['items']:  # No more users, stop
                break

        else:
            print(f"Failed to fetch data: {response.status_code}")
            break
        page += 1

    return users_data, repos_data

def fetch_user_repositories(user_login, repos_url, headers):
    """Fetch up to 500 most recently pushed repositories for a user, paginated."""
    repos_data = []
    page = 1

    while page <= 5:  # Up to 500 repos (5 pages * 100 per page)
        url = f"{repos_url}?per_page=100&page={page}"
        response = requests.get(url, headers=headers)

        if response.status_code == 200:
            repos = response.json()
            if not repos:
                break  # No more repositories, stop

            for repo in repos:
                # Handle 'license' field gracefully
                license_name = repo.get('license', {}).get('name', '') if repo.get('license') else ''

                repos_data.append([
                    user_login,
                    repo.get('full_name', ''),
                    repo.get('created_at', ''),
                    repo.get('stargazers_count', 0),
                    repo.get('watchers_count', 0),
                    repo.get('language', ''),
                    repo.get('has_projects', False),
                    repo.get('has_wiki', False),
                    license_name  # Safely extracted license name
                ])
        else:
            print(f"Failed to fetch repositories for {user_login}: {response.status_code}")
            break  # Stop if any page fails

        page += 1

    return repos_data

# Main Code
token = api_key  # Optional, for higher rate limits
users_data, repos_data = search_github_users("Hyderabad", 50, token)

if users_data and repos_data:
    # Write users data to users.csv
    with open('users.csv', 'w', newline='') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow([
            'login', 'name', 'company', 'location', 'email',
            'hireable', 'bio', 'public_repos', 'followers',
            'following', 'created_at'
        ])
        writer.writerows(users_data)

    # Write repositories data to repositories.csv
    with open('repositories.csv', 'w', newline='') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow([
            'login', 'full_name', 'created_at', 'stargazers_count',
            'watchers_count', 'language', 'has_projects',
            'has_wiki', 'license_name'
        ])
        writer.writerows(repos_data)

    print("CSV files created successfully!")
else:
    print("No data to write to CSV files.")
